{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c5813a6",
   "metadata": {},
   "source": [
    "# Data Migration: SQL Server to Postgres\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "26e59364",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import pyodbc\n",
    "import psycopg2\n",
    "from psycopg2.extras import execute_values\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf6dd32f",
   "metadata": {},
   "source": [
    "## 1. Load Credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "429b3de1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "a9270822",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_host = os.getenv(\"SQL_SERVER_HOST\")\n",
    "sql_db = os.getenv(\"SQL_SERVER_DATABASE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "b96a1649",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SQL Server Host: DESKTOP-516QCDP\\SQLEXPRESS\n",
      "SQL Server Database: TransactionDB_UAT\n"
     ]
    }
   ],
   "source": [
    "print(f\"SQL Server Host: {sql_host}\")\n",
    "print(f\"SQL Server Database: {sql_db}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "eb9d57d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pg_host = os.getenv(\"POSTGRES_HOST\")\n",
    "pg_db = os.getenv(\"POSTGRES_DATABASE\")\n",
    "pg_user = os.getenv(\"POSTGRES_USER\")\n",
    "pg_password = os.getenv(\"POSTGRES_PASSWORD\")\n",
    "pg_port = os.getenv(\"POSTGRES_PORT\")  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "b8c9444b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POSTGRES Host: localhost\n",
      "POSTGRES Database: Transaction_uat\n",
      "POSTGRES User: postgres\n",
      "POSTGRES Password: qwertyuiop\n",
      "POSTGRES Port: 5432\n"
     ]
    }
   ],
   "source": [
    "print(f\"POSTGRES Host: {pg_host}\")\n",
    "print(f\"POSTGRES Database: {pg_db}\")\n",
    "print(f\"POSTGRES User: {pg_user}\")\n",
    "print(f\"POSTGRES Password: {pg_password}\")\n",
    "print(f\"POSTGRES Port: {pg_port}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa22d3f",
   "metadata": {},
   "source": [
    "## 2 Connect to SQL SERVER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "7a36d8b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to SQL Server...\n",
      "  Server: DESKTOP-516QCDP\\SQLEXPRESS, Database: TransactionDB_UAT\n"
     ]
    }
   ],
   "source": [
    "print(\"Connecting to SQL Server...\")\n",
    "print(f\"  Server: {sql_host}, Database: {sql_db}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "67079adf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully connected to SQL Server.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    sql_conn_string = (\n",
    "        f\"DRIVER={{ODBC Driver 17 for SQL Server}};\"\n",
    "        f\"SERVER=localhost,1433;\"\n",
    "        f\"DATABASE={sql_db};\"\n",
    "        \"Trusted_Connection=yes;\"\n",
    "        \"TrustServerCertificate=yes;\"\n",
    "    )\n",
    "    sql_conn = pyodbc.connect(sql_conn_string)\n",
    "    sql_cursor = sql_conn.cursor()\n",
    "    print(\"Successfully connected to SQL Server.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"Error connecting to SQL Server:\")\n",
    "    print(e)\n",
    "    print(\"\"\" How to Troubleshoot:\n",
    "        > 1. Ensure that the SQL Server is running and accessible from your network.\n",
    "        > 2. Check server name in .env file is correct.\n",
    "        > 3. Check windows authentication permissions are enabled.\n",
    "          ....\n",
    " \"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e2948b8",
   "metadata": {},
   "source": [
    "## 3. Connect to PostgreSQL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "2d1b7a53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to Postgres...\n",
      "  Server: localhost, Database: Transaction_uat, User: postgres\n"
     ]
    }
   ],
   "source": [
    "print(\"Connecting to Postgres...\")\n",
    "print(f\"  Server: {pg_host}, Database: {pg_db}, User: {pg_user}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "4c6b6ab9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully connected to PostgreSQL.\n",
      "PostgreSQL version: PostgreSQL 18.1 on x86_64-windows, compiled by msv...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    pg_conn = psycopg2.connect(\n",
    "        host=pg_host,\n",
    "        port=pg_port,\n",
    "        dbname=pg_db,\n",
    "        user=pg_user,\n",
    "        password=pg_password\n",
    "    )\n",
    "    pg_cursor = pg_conn.cursor()\n",
    "    pg_cursor.execute(\"SELECT version();\")\n",
    "\n",
    "    pg_version = pg_cursor.fetchone()[0]\n",
    "    print(\"Successfully connected to PostgreSQL.\")\n",
    "    print(f\"PostgreSQL version: {pg_version[:50]}...\\n\")\n",
    "\n",
    "except psycopg2.OperationalError as e:\n",
    "    print(f\"Error connecting to PostgreSQL: {e}\")\n",
    "    print(\"\"\" How to Troubleshoot:\n",
    "        > 1. Ensure that the PostgreSQL server is running.\n",
    "        > 2. Check connection parameters in the .env file.\n",
    "        > 3. Ensure the user has access to the database.\n",
    "        > 4. Ensure PostgreSQL is listening on the specified port.\n",
    "    \"\"\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred while connecting to PostgreSQL: {e}\")\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129d6cb4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "75ac3d26",
   "metadata": {},
   "source": [
    "## 4. Define the tables for Migration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d548719b",
   "metadata": {},
   "source": [
    "### Migration Order\n",
    "\n",
    "- Categories (no dependencies)\n",
    "- Suppliers (no dependencies)\n",
    "- Customers (no dependencies)\n",
    "- Products (depends on Categories and Suppliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "1322b7c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Categories', 'Suppliers', 'Customers', 'Products']\n"
     ]
    }
   ],
   "source": [
    "tables_to_migrate = [\"Categories\", \"Suppliers\", \"Customers\", \"Products\"]\n",
    "print(tables_to_migrate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "9387fcff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table to migrate:\n",
      "  1: Categories\n",
      "  2: Suppliers\n",
      "  3: Customers\n",
      "  4: Products\n",
      "Total number of tables to migrate: 4\n"
     ]
    }
   ],
   "source": [
    "print(\"Table to migrate:\")\n",
    "for i, table in enumerate(tables_to_migrate, start=1):\n",
    "    print(f\"  {i}: {table}\")\n",
    "\n",
    "total_no_of_tables = len(tables_to_migrate)\n",
    "print(f\"Total number of tables to migrate: {total_no_of_tables}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a323fe2d",
   "metadata": {},
   "source": [
    "\n",
    "## 5. Run pre-migration Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "73bffc79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      ">>> CHECK 1: ROW COUNT CHECKS <<<\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 50)\n",
    "print(\">>> CHECK 1: ROW COUNT CHECKS <<<\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d3ab35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "a8f93f04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categories      :            8 rows\n",
      "Suppliers       :         5000 rows\n",
      "Customers       :       900000 rows\n",
      "Products        :       150000 rows\n",
      "==================================================\n",
      "Total Rows to Migrate     :       1,055,008 rows\n",
      "\n",
      " Baseline Captured! \n"
     ]
    }
   ],
   "source": [
    "baseline_row_counts = {}\n",
    "\n",
    "try:\n",
    "    for table in tables_to_migrate:\n",
    "\n",
    "        row_count_query = f\"\"\"\n",
    "        SELECT SUM(row_count)\n",
    "        FROM sys.dm_db_partition_stats\n",
    "        WHERE object_id = OBJECT_ID('{table}')\n",
    "          AND index_id IN (0,1)\n",
    "        \"\"\"\n",
    "\n",
    "        sql_cursor.execute(row_count_query)\n",
    "        count = sql_cursor.fetchone()[0] or 0\n",
    "\n",
    "        key = table.lower()\n",
    "        baseline_row_counts[key] = count\n",
    "\n",
    "        print(f\"{table:15} : {count: >12} rows\")\n",
    "\n",
    "    total_rows = sum(baseline_row_counts.values())\n",
    "\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"{'Total Rows to Migrate':<25} : {total_rows:>15,} rows\")\n",
    "    print(\"\\n Baseline Captured! \")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Baseline capture failed:{e}\")\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "f43bd3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_cursor.execute(\"\"\"\n",
    "    SELECT\n",
    "        SCHEMA_NAME(schema_id) + '.' + name AS object_name,\n",
    "        QUOTENAME(SCHEMA_NAME(schema_id)) + '.' + QUOTENAME(name) AS quoted_name\n",
    "    FROM sys.tables\n",
    "    WHERE is_ms_shipped = 0\n",
    "\"\"\")\n",
    "\n",
    "table_map = {\n",
    "    row[0]: row[1]   # unquoted_name -> quoted_name\n",
    "    for row in sql_cursor.fetchall()\n",
    "}\n",
    "\n",
    "tables_to_migrate = list(table_map.keys())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "d86b3cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not tables_to_migrate:\n",
    "    raise RuntimeError(\"No user tables found in database.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "24701f35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      ">>> CHECK 2: NULL COUNTS (CustomerName) <<<\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 50)\n",
    "print(\">>> CHECK 2: NULL COUNTS (CustomerName) <<<\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "311137ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== STARTING DATA QUALITY CHECKS ===\n",
      "\n",
      "CHECK 2: NULL Customer Names\n",
      "\n",
      "CHECK 3: Invalid Email Formats\n",
      "\n",
      "CHECK 4: Negative Product Prices\n",
      "\n",
      "CHECK 5: Negative Stock Quantities\n",
      "\n",
      "CHECK 6: Orphaned Supplier References\n",
      "\n",
      "CHECK 7: Future-Dated Customer Records\n",
      "\n",
      "=== DATA QUALITY REPORT ===\n",
      "Data quality issues detected (Will Migrate as is):\n",
      "- 4,514 customers with NULL names\n",
      "- 27,318 customers with invalid email formats\n",
      "- 775 prices with negative prices\n",
      "- 1,467 products with negative stock quantities\n",
      "- 24,700 products with orphaned supplier IDs\n",
      "- 2,731 customers with future-dated CreatedDate values\n"
     ]
    }
   ],
   "source": [
    "quality_issues = []\n",
    "\n",
    "try:\n",
    "    print(\"\\n=== STARTING DATA QUALITY CHECKS ===\")\n",
    "\n",
    "    # -------------------------------\n",
    "    # CHECK 2: NULL Customer Names\n",
    "    # -------------------------------\n",
    "    print(\"\\nCHECK 2: NULL Customer Names\")\n",
    "\n",
    "    sql_cursor.execute(\"\"\"\n",
    "        SELECT COUNT(*)\n",
    "        FROM Customers\n",
    "        WHERE CustomerName IS NULL\n",
    "    \"\"\")\n",
    "\n",
    "    null_names = sql_cursor.fetchone()[0]\n",
    "\n",
    "    if null_names > 0:\n",
    "        quality_issues.append(\n",
    "            f\"{null_names:,} customers with NULL names\"\n",
    "        )\n",
    "\n",
    "\n",
    "    # -------------------------------\n",
    "    # CHECK 3: Invalid Emails\n",
    "    # -------------------------------\n",
    "    print(\"\\nCHECK 3: Invalid Email Formats\")\n",
    "\n",
    "    email_sql = \"\"\"\n",
    "        SELECT COUNT(*)\n",
    "        FROM Customers\n",
    "        WHERE Email IS NOT NULL\n",
    "        AND (\n",
    "            Email NOT LIKE '%_@_%._%'\n",
    "            OR Email LIKE '%@invalid%'\n",
    "            OR Email LIKE '% %'\n",
    "            OR Email LIKE '%..%'\n",
    "            OR Email LIKE '%.@%'\n",
    "            OR Email LIKE '%@.%'\n",
    "            OR Email LIKE '@%'\n",
    "            OR Email LIKE '%.'\n",
    "        )\n",
    "    \"\"\"\n",
    "\n",
    "    sql_cursor.execute(email_sql)\n",
    "\n",
    "    invalid_emails = sql_cursor.fetchone()[0]\n",
    "\n",
    "    if invalid_emails > 0:\n",
    "        quality_issues.append(\n",
    "            f\"{invalid_emails:,} customers with invalid email formats\"\n",
    "        )\n",
    "\n",
    "\n",
    "    # -------------------------------\n",
    "    # CHECK 4: Negative Prices\n",
    "    # -------------------------------\n",
    "    print(\"\\nCHECK 4: Negative Product Prices\")\n",
    "\n",
    "    sql_cursor.execute(\"\"\"\n",
    "        SELECT COUNT(*)\n",
    "        FROM Products\n",
    "        WHERE UnitPrice < 0\n",
    "    \"\"\")\n",
    "\n",
    "    negative_prices = sql_cursor.fetchone()[0]\n",
    "\n",
    "    if negative_prices > 0:\n",
    "        quality_issues.append(\n",
    "            f\"{negative_prices:,} prices with negative prices\"\n",
    "        )\n",
    "\n",
    "\n",
    "    # -------------------------------\n",
    "    # CHECK 5: Negative Stock\n",
    "    # -------------------------------\n",
    "    print(\"\\nCHECK 5: Negative Stock Quantities\")\n",
    "\n",
    "    sql_cursor.execute(\"\"\"\n",
    "        SELECT COUNT(*)\n",
    "        FROM Products\n",
    "        WHERE StockQuantity < 0\n",
    "    \"\"\")\n",
    "\n",
    "    negative_stock = sql_cursor.fetchone()[0]\n",
    "\n",
    "    if negative_stock > 0:\n",
    "        quality_issues.append(\n",
    "            f\"{negative_stock:,} products with negative stock quantities\"\n",
    "        )\n",
    "\n",
    "\n",
    "    # -------------------------------\n",
    "    # CHECK 6: Orphaned Foreign Keys\n",
    "    # -------------------------------\n",
    "    print(\"\\nCHECK 6: Orphaned Supplier References\")\n",
    "\n",
    "    sql_cursor.execute(\"\"\"\n",
    "        SELECT COUNT(*)\n",
    "        FROM Products prod\n",
    "        WHERE NOT EXISTS (\n",
    "            SELECT 1\n",
    "            FROM Suppliers sup\n",
    "            WHERE sup.SupplierID = prod.SupplierID\n",
    "        )\n",
    "    \"\"\")\n",
    "\n",
    "    orphaned_fks = sql_cursor.fetchone()[0]\n",
    "\n",
    "    if orphaned_fks > 0:\n",
    "        quality_issues.append(\n",
    "            f\"{orphaned_fks:,} products with orphaned supplier IDs\"\n",
    "        )\n",
    "\n",
    "    # -------------------------------\n",
    "    # CHECK 7: Future-Dated Records\n",
    "    # -------------------------------\n",
    "    print(\"\\nCHECK 7: Future-Dated Customer Records\")\n",
    "\n",
    "    sql_cursor.execute(\"\"\"\n",
    "        SELECT COUNT(*)\n",
    "        FROM Customers\n",
    "        WHERE CreatedDate > GETDATE()\n",
    "    \"\"\")\n",
    "\n",
    "    future_dates = sql_cursor.fetchone()[0]\n",
    "\n",
    "    if future_dates > 0:\n",
    "        quality_issues.append(\n",
    "            f\"{future_dates:,} customers with future-dated CreatedDate values\"\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "    # -------------------------------\n",
    "    # FINAL REPORT\n",
    "    # -------------------------------\n",
    "    print(\"\\n=== DATA QUALITY REPORT ===\")\n",
    "\n",
    "    if not quality_issues:\n",
    "        print(\"No data quality issues found.\")\n",
    "    else:\n",
    "        print(\"Data quality issues detected (Will Migrate as is):\")\n",
    "        for issue in quality_issues:\n",
    "            print(f\"- {issue}\")\n",
    "\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"Data quality check failed:\")\n",
    "    print(e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce6fe8c8",
   "metadata": {},
   "source": [
    "## 6. Get Table Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "649b746a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      ">>> ANALYZE TABLE SCHEMA <<<\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 50)\n",
    "print(\">>> ANALYZE TABLE SCHEMA <<<\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "759e181d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Daniel\\AppData\\Local\\Temp\\ipykernel_31936\\1417242039.py:20: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  schema_df = pd.read_sql(schema_query, sql_conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded schema for: Categories\n",
      "Loaded schema for: Suppliers\n",
      "Loaded schema for: Customers\n",
      "Loaded schema for: Products\n"
     ]
    }
   ],
   "source": [
    "table_schema = {}\n",
    "\n",
    "try:\n",
    "    for table in tables_to_migrate:\n",
    "\n",
    "        clean_table = table.split(\".\")[-1]\n",
    "\n",
    "        schema_query = f\"\"\"\n",
    "            SELECT\n",
    "                COLUMN_NAME,\n",
    "                DATA_TYPE,\n",
    "                CHARACTER_MAXIMUM_LENGTH,\n",
    "                IS_NULLABLE\n",
    "            FROM INFORMATION_SCHEMA.COLUMNS\n",
    "            WHERE TABLE_NAME = '{clean_table}'\n",
    "              AND TABLE_SCHEMA = 'dbo'\n",
    "            ORDER BY ORDINAL_POSITION\n",
    "        \"\"\"\n",
    "\n",
    "        schema_df = pd.read_sql(schema_query, sql_conn)\n",
    "\n",
    "        if schema_df.empty:\n",
    "            print(f\"Warning: No schema found for {clean_table}\")\n",
    "        else:\n",
    "            table_schema[clean_table] = schema_df\n",
    "            print(f\"Loaded schema for: {clean_table}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"Schema extraction failed:\")\n",
    "    print(e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "093f1600",
   "metadata": {},
   "source": [
    "## 7. Define Data Type Mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "0f31daa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      ">>> DATA TYPE MAPPING <<<\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 50)\n",
    "print(\">>> DATA TYPE MAPPING <<<\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "30aa3779",
   "metadata": {},
   "outputs": [],
   "source": [
    "type_mapping = {\n",
    "    \"int\": \"INTEGER\",\n",
    "    \"bigint\": \"BIGINT\",\n",
    "    \"smallint\": \"SMALLINT\",\n",
    "    \"tinyint\": \"SMALLINT\",\n",
    "    \"bit\": \"BOOLEAN\",\n",
    "    \"decimal\": \"NUMERIC\",\n",
    "    \"numeric\": \"NUMERIC\",   \n",
    "    \"money\": \"NUMERIC(19,4)\",\n",
    "    \"smallmoney\": \"NUMERIC(10,4)\",\n",
    "    \"float\": \"DOUBLE PRECISION\",\n",
    "    \"real\": \"REAL\",\n",
    "    \"datetime\": \"TIMESTAMP\",\n",
    "    \"datetime2\": \"TIMESTAMP\",\n",
    "    \"smalldatetime\": \"TIMESTAMP\",\n",
    "    \"date\": \"DATE\",\n",
    "    \"time\": \"TIME\",\n",
    "    \"char\": \"CHAR\",\n",
    "    \"varchar\": \"VARCHAR\",\n",
    "    \"nchar\": \"CHAR\",    \n",
    "    \"nvarchar\": \"VARCHAR\",  \n",
    "    \"text\": \"TEXT\",\n",
    "    \"ntext\": \"TEXT\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "fd83e2af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SQL Server to Postgres Data Type Mapping:\n",
      "\n",
      "  int               -->    INTEGER\n",
      "  bigint            -->    BIGINT\n",
      "  smallint          -->    SMALLINT\n",
      "  tinyint           -->    SMALLINT\n",
      "  bit               -->    BOOLEAN\n",
      "  decimal           -->    NUMERIC\n",
      "  numeric           -->    NUMERIC\n",
      "  money             -->    NUMERIC(19,4)\n",
      "  smallmoney        -->    NUMERIC(10,4)\n",
      "  float             -->    DOUBLE PRECISION\n",
      "  real              -->    REAL\n",
      "  datetime          -->    TIMESTAMP\n",
      "  datetime2         -->    TIMESTAMP\n",
      "  smalldatetime     -->    TIMESTAMP\n",
      "  date              -->    DATE\n",
      "  time              -->    TIME\n",
      "  char              -->    CHAR\n",
      "  varchar           -->    VARCHAR\n",
      "  nchar             -->    CHAR\n",
      "  nvarchar          -->    VARCHAR\n",
      "  text              -->    TEXT\n",
      "  ntext             -->    TEXT\n"
     ]
    }
   ],
   "source": [
    "print(\"SQL Server to Postgres Data Type Mapping:\")\n",
    "print()\n",
    "\n",
    "for sql_type, pg_type in list(type_mapping.items()):\n",
    "    print(f\"  {sql_type:<17} -->    {pg_type}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf281274",
   "metadata": {},
   "source": [
    "## 8. Create tables In PostgreSQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "8e4dfa54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      ">>> CREATE TABLES IN POSTGRES <<<\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 50)\n",
    "print(\">>> CREATE TABLES IN POSTGRES <<<\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "5517ff99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SUCCESS] Created table: categories\n",
      "[SUCCESS] Created table: suppliers\n",
      "[SUCCESS] Created table: customers\n",
      "[SUCCESS] Created table: products\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    for table in tables_to_migrate:\n",
    "\n",
    "        # Remove dbo. prefix\n",
    "        clean_table = table.replace(\"dbo.\", \"\")\n",
    "\n",
    "        schema = table_schema[clean_table]\n",
    "\n",
    "        pg_table = clean_table.lower()\n",
    "\n",
    "        pg_cursor.execute(f\"DROP TABLE IF EXISTS {pg_table} CASCADE;\")\n",
    "\n",
    "        column_definitions = []\n",
    "\n",
    "        for idx, row in schema.iterrows():\n",
    "\n",
    "            col_name = row[\"COLUMN_NAME\"].lower()\n",
    "            sql_type = row[\"DATA_TYPE\"]\n",
    "            is_nullable = row[\"IS_NULLABLE\"]\n",
    "\n",
    "            base_type = sql_type.lower()\n",
    "            pg_type = type_mapping.get(base_type, \"TEXT\")\n",
    "\n",
    "            condition_1 = idx == 0\n",
    "            condition_2 = col_name.endswith(\"id\")\n",
    "            condition_3 = \"int\" in base_type\n",
    "\n",
    "            if condition_1 and condition_2 and condition_3:\n",
    "                column_definitions.append(\n",
    "                    f\"{col_name} SERIAL PRIMARY KEY\"\n",
    "                )\n",
    "            else:\n",
    "                col_def = f\"{col_name} {pg_type}\"\n",
    "\n",
    "                if is_nullable == \"NO\":\n",
    "                    col_def += \" NOT NULL\"\n",
    "\n",
    "                column_definitions.append(col_def)\n",
    "\n",
    "\n",
    "        column_string = \",\\n      \".join(column_definitions)\n",
    "\n",
    "        create_query = f\"\"\"\n",
    "        CREATE TABLE {pg_table} (\n",
    "              {column_string}\n",
    "        );\n",
    "        \"\"\"\n",
    "\n",
    "        pg_cursor.execute(create_query)\n",
    "        pg_conn.commit()\n",
    "\n",
    "        print(f\"[SUCCESS] Created table: {pg_table}\")\n",
    "\n",
    "\n",
    "except psycopg2.Error as e:\n",
    "    print(f\"Postgres experienced an error while creating a table: {e}\")\n",
    "    pg_conn.rollback()\n",
    "    raise\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error creating tables in Postgres: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab1fcd0",
   "metadata": {},
   "source": [
    "### Testing Migration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "6b1cb0ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      ">>> TESTING MIGRATION (SINGLE TABLE) <<<\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 50)\n",
    "print(\">>> TESTING MIGRATION (SINGLE TABLE) <<<\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "c50206d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_table = 'customers'  # Change this to test different tables\n",
    "pg_table = test_table.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "bbf6f0dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Reading data from SQL Server...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Daniel\\AppData\\Local\\Temp\\ipykernel_31936\\3024234870.py:8: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  test_df = pd.read_sql(extract_query, sql_conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Retrieved 900,000 rows from customers.\n",
      "2. Transforming data types...\n",
      "[SUCCESS] Converted IsActive to boolean.\n",
      "3. Preparing data for loading...\n",
      "        Prepared 900,000 rows.\n",
      "4. Clearing existing data (safety)...\n",
      "5. Loading data into Postgres...\n",
      "[SUCCESS] Loaded 900,000 rows into customers.\n",
      "6. Verifying row counts...\n",
      "[SUCCESS] Row count matches: 900,000 == 900,000\n",
      "\n",
      "Migration for table 'customers' completed successfully.\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    print(\"1. Reading data from SQL Server...\")\n",
    "\n",
    "    source_table = test_table.split(\".\")[-1]\n",
    "    pg_table = source_table.lower()\n",
    "\n",
    "    extract_query = f\"SELECT * FROM {test_table}\"\n",
    "    test_df = pd.read_sql(extract_query, sql_conn)\n",
    "\n",
    "    print(f\"        Retrieved {len(test_df):,} rows from {test_table}.\")\n",
    "\n",
    "    print(\"2. Transforming data types...\")\n",
    "\n",
    "    for col in test_df.columns:\n",
    "        if col.lower() == \"isactive\":\n",
    "            test_df[col] = test_df[col].astype(bool)\n",
    "            print(\"[SUCCESS] Converted IsActive to boolean.\")\n",
    "\n",
    "    print(\"3. Preparing data for loading...\")\n",
    "\n",
    "    columns = [col.lower() for col in test_df.columns]\n",
    "    columns_string = \", \".join(columns)\n",
    "\n",
    "    data_tuples = [tuple(row) for row in test_df.to_numpy()]\n",
    "\n",
    "    print(f\"        Prepared {len(data_tuples):,} rows.\")\n",
    "\n",
    "    print(\"4. Clearing existing data (safety)...\")\n",
    "\n",
    "    pg_cursor.execute(f\"TRUNCATE TABLE {pg_table} RESTART IDENTITY CASCADE;\")\n",
    "    pg_conn.commit()\n",
    "\n",
    "    print(\"5. Loading data into Postgres...\")\n",
    "\n",
    "    insert_query = f\"\"\"\n",
    "        INSERT INTO {pg_table} ({columns_string})\n",
    "        VALUES %s\n",
    "    \"\"\"\n",
    "\n",
    "    execute_values(pg_cursor, insert_query, data_tuples, page_size=1000)\n",
    "    pg_conn.commit()\n",
    "\n",
    "    print(f\"[SUCCESS] Loaded {len(data_tuples):,} rows into {pg_table}.\")\n",
    "\n",
    "    print(\"6. Verifying row counts...\")\n",
    "\n",
    "    pg_cursor.execute(f\"SELECT COUNT(*) FROM {pg_table}\")\n",
    "    pg_count = pg_cursor.fetchone()[0]\n",
    "\n",
    "    baseline_key = source_table\n",
    "\n",
    "    if baseline_key in baseline_row_counts:\n",
    "        sql_count = baseline_row_counts[baseline_key]\n",
    "\n",
    "        if pg_count == sql_count:\n",
    "            print(f\"[SUCCESS] Row count matches: {pg_count:,} == {sql_count:,}\")\n",
    "        else:\n",
    "            print(f\"[ERROR] Row count mismatch: {pg_count:,} != {sql_count:,}\")\n",
    "    else:\n",
    "        print(f\"[WARNING] No baseline found for {baseline_key}\")\n",
    "\n",
    "    print(f\"\\nMigration for table '{test_table}' completed successfully.\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "except Exception as e:\n",
    "    pg_conn.rollback()\n",
    "    print(\"\\nMigration failed:\")\n",
    "    print(e)\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54828202",
   "metadata": {},
   "source": [
    "## 10. Migrating rremaining tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "594b8baa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      ">>> MIGRATING REMAINING TABLES <<<\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 50)\n",
    "print(\">>> MIGRATING REMAINING TABLES <<<\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "b05488a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Migrating table: dbo.Categories  --->  categories\n",
      "1. Reading data from SQL Server...\n",
      "        Retrieved 8 rows from dbo.Categories.\n",
      "2. Preparing data types...\n",
      "       Prepared 8 rows for insertion.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Daniel\\AppData\\Local\\Temp\\ipykernel_31936\\3617544266.py:18: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  sql_df = pd.read_sql(extract_query, sql_conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "3. Clearing existing data...\n",
      "4. Processing bulk load...\n",
      "[SUCCESS] ------> Loaded 8 rows into categories.\n",
      "\n",
      "5. Verifying row counts...\n",
      "[SUCCESS] -----> Verification passed: 8 == 8\n",
      "\n",
      "Migration for table 'dbo.Categories' completed successfully.\n",
      "Migrating table: dbo.Suppliers  --->  suppliers\n",
      "1. Reading data from SQL Server...\n",
      "        Retrieved 5,000 rows from dbo.Suppliers.\n",
      "2. Preparing data types...\n",
      "       Prepared 5,000 rows for insertion.\n",
      "\n",
      "3. Clearing existing data...\n",
      "4. Processing bulk load...\n",
      "[SUCCESS] ------> Loaded 5,000 rows into suppliers.\n",
      "\n",
      "5. Verifying row counts...\n",
      "[SUCCESS] -----> Verification passed: 5,000 == 5,000\n",
      "\n",
      "Migration for table 'dbo.Suppliers' completed successfully.\n",
      "Migrating table: dbo.Products  --->  products\n",
      "1. Reading data from SQL Server...\n",
      "        Retrieved 150,000 rows from dbo.Products.\n",
      "2. Preparing data types...\n",
      "       Prepared 150,000 rows for insertion.\n",
      "\n",
      "3. Clearing existing data...\n",
      "4. Processing bulk load...\n",
      "[SUCCESS] ------> Loaded 150,000 rows into products.\n",
      "\n",
      "5. Verifying row counts...\n",
      "[SUCCESS] -----> Verification passed: 150,000 == 150,000\n",
      "\n",
      "Migration for table 'dbo.Products' completed successfully.\n"
     ]
    }
   ],
   "source": [
    "remaining_tables = [\n",
    "    t for t in tables_to_migrate\n",
    "    if t.split(\".\")[-1].lower() != \"customers\"\n",
    "]\n",
    "\n",
    "\n",
    "for table in remaining_tables:\n",
    "\n",
    "    clean_table = table.split(\".\")[-1]\n",
    "    pg_table = clean_table.lower()\n",
    "\n",
    "    print(f\"Migrating table: {table}  --->  {pg_table}\")\n",
    "\n",
    "    try:\n",
    "        print(\"1. Reading data from SQL Server...\")\n",
    "\n",
    "        extract_query = f\"SELECT * FROM {table}\"\n",
    "        sql_df = pd.read_sql(extract_query, sql_conn)\n",
    "\n",
    "        print(f\"        Retrieved {len(sql_df):,} rows from {table}.\")\n",
    "\n",
    "\n",
    "        print(\"2. Preparing data types...\")\n",
    "\n",
    "        data_tuples = [tuple(row) for row in sql_df.to_numpy()]\n",
    "        columns = [col.lower() for col in sql_df.columns]\n",
    "\n",
    "        column_string = \", \".join(columns)\n",
    "\n",
    "        insert_query = f\"\"\"\n",
    "            INSERT INTO {pg_table} ({column_string})\n",
    "            VALUES %s\n",
    "        \"\"\"\n",
    "\n",
    "        print(f\"       Prepared {len(data_tuples):,} rows for insertion.\\n\")\n",
    "\n",
    "\n",
    "        print(\"3. Clearing existing data...\")\n",
    "\n",
    "        pg_cursor.execute(\n",
    "            f\"TRUNCATE TABLE {pg_table} RESTART IDENTITY CASCADE;\"\n",
    "        )\n",
    "        pg_conn.commit()\n",
    "\n",
    "\n",
    "        print(\"4. Processing bulk load...\")\n",
    "\n",
    "        execute_values(\n",
    "            pg_cursor,\n",
    "            insert_query,\n",
    "            data_tuples,\n",
    "            page_size=1000\n",
    "        )\n",
    "\n",
    "        pg_conn.commit()\n",
    "\n",
    "        print(f\"[SUCCESS] ------> Loaded {len(data_tuples):,} rows into {pg_table}.\\n\")\n",
    "\n",
    "\n",
    "        print(\"5. Verifying row counts...\")\n",
    "\n",
    "        pg_cursor.execute(f\"SELECT COUNT(*) FROM {pg_table}\")\n",
    "        pg_count = pg_cursor.fetchone()[0]\n",
    "\n",
    "\n",
    "        baseline_key = clean_table.lower()\n",
    "\n",
    "        sql_count = baseline_row_counts.get(baseline_key)\n",
    "\n",
    "\n",
    "        if sql_count is None:\n",
    "            print(f\"[WARNING] No baseline found for {baseline_key}\\n\")\n",
    "\n",
    "        elif pg_count == sql_count:\n",
    "            print(f\"[SUCCESS] -----> Verification passed: {pg_count:,} == {sql_count:,}\\n\")\n",
    "\n",
    "        else:\n",
    "            print(f\"[ERROR] ------> Count mismatch: {pg_count:,} != {sql_count:,}\\n\")\n",
    "\n",
    "\n",
    "        print(f\"Migration for table '{table}' completed successfully.\")\n",
    "\n",
    "\n",
    "    except Exception as e:\n",
    "\n",
    "        pg_conn.rollback()\n",
    "\n",
    "        print(f\"\\nMigration failed for table '{table}':\")\n",
    "        print(e)\n",
    "\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d59e32cb",
   "metadata": {},
   "source": [
    "## 11. Run post-migration validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3482b1dd",
   "metadata": {},
   "source": [
    "### 1. Row count in postgres\n",
    "\n",
    "##### SUCCESS: MATCHES SQL SERVER\n",
    "\n",
    "### 2. Data type checks\n",
    "\n",
    "##### SUCCESS: MATCHES SQL SERVER\n",
    "\n",
    "### 3. Primary Key checks (duplicates)\n",
    "\n",
    "##### SUCCESS: MATCHES SQL SERVER\n",
    "\n",
    "### 4. Referential integrity checks \n",
    "\n",
    "##### SUCCESS: MATCHES SQL SERVER.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a762111f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ad8c24",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sqlserver-to-postgres",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
